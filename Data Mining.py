# -*- coding: utf-8 -*-
"""exposé data mining _Exams.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NyfWlKONF6gEIXNteB_TSQQaps1AHZFi
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Exploratory Data Analysis (EDA)**"""

data = pd.read_csv("/content/drive/MyDrive/exams.csv")
data.head()

data

data.shape

data.columns

data.isnull().sum()  #count the number of missing (null) values in each column of a DataFrame.

data.info()     #a concise summary of the DataFrame's structure

"""we have 5 attributes of type object and the rest is  integer. There's no null values too."""

data.dtypes   #data types of each column

data.duplicated

data.nunique()  #checking for the unique values in each attribute.

data.describe()

data['gender'].value_counts()

data['parental level of education'].value_counts()

data['race/ethnicity'].value_counts()

data['lunch'].value_counts()

data['test preparation course'].value_counts()

import pandas as pd


dominant_values = data['math score'].value_counts()
print(dominant_values)
dominant_value = data['math score'].mode().values[0]
print("Dominant value:", dominant_value)
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(data['math score'], kde=True)
plt.show()
mean = data['math score'].mean()
median = data['math score'].median()
mode = data['math score'].mode().values[0]

print("Mean:", mean)
print("Median:", median)
print("Mode:", mode)

# People with top 15% math score
top_15_math = data['math score'] > data['math score'].quantile(q=0.85)

top_15_math_math_mean = data[top_15_math]['math score'].mean()
top_15_math_writing_mean = data[top_15_math]['writing score'].mean()
top_15_math_reading_mean = data[top_15_math]['reading score'].mean()


# People with top 15% writing score
top_15_writing = data['writing score'] > data['writing score'].quantile(q=0.85)

top_15_writing_math_mean = data[top_15_writing]['math score'].mean()
top_15_writing_writing_mean = data[top_15_writing]['writing score'].mean()
top_15_writing_reading_mean = data[top_15_writing]['reading score'].mean()

# People with top 15% reading score
top_15_reading = data['reading score'] > data['reading score'].quantile(q=0.85) # People with top 15% reading score

top_15_reading_math_mean = data[top_15_reading]['math score'].mean()
top_15_reading_writing_mean = data[top_15_reading]['writing score'].mean()
top_15_reading_reading_mean = data[top_15_reading]['reading score'].mean()

# Summary DataFrame
compare_data = pd.DataFrame({
    'Math': [top_15_math_math_mean, top_15_writing_math_mean, top_15_reading_math_mean],
    'Writing': [top_15_math_writing_mean, top_15_writing_writing_mean, top_15_reading_writing_mean],
    'Rading': [top_15_math_reading_mean, top_15_writing_reading_mean, top_15_reading_reading_mean]
}, index=['Top 15% math score', 'Top 15% writing score', 'Top 15% reading score'])

compare_data

data['total']=data['math score']+data['reading score']+data['writing score']

data['average']=data['total']/3

sns.distplot(data['math score'])   #distribution plot (histogram) of the 'math score'

sns.distplot(data['reading score'])

sns.distplot(data['writing score'])

sns.distplot(data['average'])

sns.pairplot(data)   #visualize the relationships between multiple variables

plt.rcParams['figure.figsize'] = (30, 12)

plt.subplot(1, 5, 1)
size = data['gender'].value_counts()
labels = 'Female', 'Male'
color = ['red','green']


plt.pie(size, colors = color, labels = labels,autopct = '.%2f%%')
plt.title('Gender', fontweight='bold')
plt.axis('off')
plt.subplot(1, 5, 2)
size = data['race/ethnicity'].value_counts()
labels = 'Group C', 'Group D','Group B','Group E','Group A'
color = ['red', 'green', 'blue', 'cyan','orange']

plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Race_Ethnicity', fontweight='bold')
plt.axis('off')
plt.subplot(1, 5, 3)
size = data['lunch'].value_counts()
labels = 'Standard', 'Free'
color = ['red','green']

plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Lunch', fontweight='bold')
plt.axis('off')
plt.subplot(1, 5, 4)
size = data['test preparation course'].value_counts()
labels = 'None', 'Completed'
color = ['red','green']

plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Test Course',fontweight='bold')
plt.axis('off')
plt.subplot(1, 5, 5)
size = data['parental level of education'].value_counts()
labels = 'Some College', "Associate's Degree",'High School','Some High School',"Bachelor's Degree","Master's Degree"
color = ['red', 'green', 'blue', 'cyan','orange','grey']
plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Parental Education', fontweight='bold')
plt.axis('off')


plt.tight_layout()
plt.grid()

plt.show()

data['race/ethnicity'][data['average']==min (data['average'])]
 #extract the 'race/ethnicity' values for the rows where the 'average'
 #column has the minimum value in your DataFrame.

data['parental level of education'][data['average']==min (data['average'])]

data['math_PassStatus']=np.where(data['math score']<40,'F','P')
# categorize students' math scores as either 'P' (pass) or 'F' (fail)
#whether the 'math score' is less than 40.

data['read_PassStatus']=np.where(data['reading score']<40,'F','P')

data['write_PassStatus']=np.where(data['writing score']<40,'F','P')

data.head()

data['math_PassStatus'].value_counts()
#count the number of occurrences of each unique value in the 'math_PassStatus' column

data['read_PassStatus'].value_counts()

data['write_PassStatus'].value_counts()

p=sns.countplot(x='parental level of education', data=data,hue='math_PassStatus',palette='bright')
_=plt.setp(p.get_xticklabels(),rotation=90)
#visualize the distribution of students' math pass status based on their parental level of education.

p=sns.countplot(x='test preparation course', data=data,hue='math_PassStatus',palette='bright')
_=plt.setp(p.get_xticklabels(),rotation=90)
#visualize the distribution of students' math pass status based on whether they completed a test preparation course.

plt.boxplot([data[data['gender'] == 'male']['math score'], data[data['gender'] == 'female']['math score']],
            labels=['Males', 'Females'])
#create a boxplot to compare the distribution
#of math scores between male and female students.

"""Diagramme de violon"""

# Créez le diagramme de violon
plt.figure(figsize=(8, 6))
sns.violinplot(x='gender', y='math score', data=data)

# Ajoutez un titre
plt.title('Diagramme de violon des scores en mathématiques par genre')

# Affichez le diagramme de violon
plt.show()

"""La corrélation entre les variables"""

# Calculer la matrice de corrélation
correlation_matrix = data.corr()

# Créer une carte de chaleur (heatmap) pour visualiser la corrélation
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

# Ajouter un titre
plt.title('Matrice de corrélation')

# Afficher la carte de chaleur
plt.show()

#rename columns
data.rename(columns = {'race/ethnicity	':'race_ethnicity	' , 'parental level of education':'parental_level_of_education' , 'test preparation course':'test_preparation_course' }, inplace = True)

"""# **Coding**

Encoding of **Gender**
"""

from sklearn.preprocessing import OneHotEncoder
#
# OneHotEncoder
#
ohe = OneHotEncoder()
#
# One-hot encoding pour transformer la varaible "gender"
#
ohe.fit_transform(data.gender.values.reshape(-1, 1)).toarray()

"""Encoding of **lunch**"""

from sklearn.preprocessing import OneHotEncoder
#
# OneHotEncoder
#
ohe = OneHotEncoder()
#
# One-hot encoding pour transformer la varaible "lunch"
#
data['lunch']=ohe.fit_transform(data.lunch.values.reshape(-1, 1)).toarray()
data['lunch']

"""encoding of **race**/**ethnicity**"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the 'race' column
data['race/ethnicity'] = label_encoder.fit_transform(data['race/ethnicity'])

# Display the resulting data frame
data['race/ethnicity']

"""encoding of **parental** **level** **of** **education**"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the 'parental level of education' column
data['parental_level_of_education'] = label_encoder.fit_transform(data['parental_level_of_education'])

# Display the resulting data frame
data['parental_level_of_education']

"""encoding **of** **test** **preparation** **course**




"""

from sklearn.preprocessing import OneHotEncoder
#
# OneHotEncoder
#
ohe = OneHotEncoder()
#
# One-hot encoding pour transformer la varaible "test preparation course"
#
data['test_preparation_course']=ohe.fit_transform(data.test_preparation_course.values.reshape(-1, 1)).toarray()
data['test_preparation_course']

from sklearn.compose import ColumnTransformer
ct = ColumnTransformer([('one-hot-encoder', OneHotEncoder(), ['gender','lunch','test_preparation_course'])], remainder='passthrough')
ct.fit_transform(data)

data

"""# **CAH**

*** Par la standarisation ***
"""

from sklearn.preprocessing import StandardScaler

# Sélectionner les colonnes numériques qu'on souhaite standardiser
numeric_cols = ['math score','reading score','writing score']

# Appliquer la standardisation z-score aux colonnes numériques
scaler = StandardScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])

# Afficher le DataFrame résultant
print(data)

import pandas as pd
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Sélectionner les colonnes numériques que nous avons standardisées
numeric_cols = ['math score','reading score','writing score']

# Appliquer la standardisation z-score aux colonnes numériques
scaler = StandardScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])

# Réaliser une analyse de clustering hiérarchique sur les données standardisées
Z = linkage(data[numeric_cols], method='ward')

# Créer le dendrogramme
plt.figure(figsize=(12, 6))
dendrogram(Z)
plt.title("Dendrogramme")
plt.xlabel("Échantillons")
plt.ylabel("Distance")
plt.show()

from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering

n_clusters = 2

agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)
data['cluster'] = agg_clustering.fit_predict(data[numeric_cols])

# Calculer l'indice de silhouette
silhouette_avg = silhouette_score(data[numeric_cols], data['cluster'])
print(f"Silhouette Score : {silhouette_avg}")

"""Indice de silhouette est raisonnable

**CAH par méthode de la distance moyenne avec la dissimilarité de Jaccard**

**-Transformer les variables quantitatives (math
score,writing score, reading score) en des variables qualitatives selon le score obtenu en des groupes A,B,C...
 -Après cette transformation , on obtient une base de données qui contient seulement des variables quantitatives .
==> Alors on fait le tableau disjonctif complet et on applique le CAH**

*Trasformation de math score*
"""

import pandas as pd

# Exemple de DataFrame avec une colonne de valeurs quantitatives

df = pd.DataFrame(data)

# Définissez les intervalles pour regrouper les valeurs quantitatives
intervals = [0, 30, 80, 150, float('inf')]
labels = ['Groupe 1', 'Groupe 2', 'Groupe 3', 'Groupe 4']

# Utilisez la méthode cut pour créer une nouvelle colonne de valeurs qualitatives
df['math score'] = pd.cut(df['math score'], bins=intervals, labels=labels)

# Affichez la DataFrame résultante
print(df)

df['math score']

"""*Transformation de reading score*"""

import pandas as pd

# Exemple de DataFrame avec une colonne de valeurs quantitatives

df = pd.DataFrame(data)

# Définissez les intervalles pour regrouper les valeurs quantitatives
intervals = [0, 30, 80, 150, float('inf')]
labels = ['Groupe 1', 'Groupe 2', 'Groupe 3', 'Groupe 4']

# Utilisez la méthode cut pour créer une nouvelle colonne de valeurs qualitatives
df['reading score'] = pd.cut(df['reading score'], bins=intervals, labels=labels)

# Affichez la DataFrame résultante
print(df)

df['reading score']

"""*Transformation de writing score*"""

import pandas as pd

# Exemple de DataFrame avec une colonne de valeurs quantitatives

df = pd.DataFrame(data)

# Définissez les intervalles pour regrouper les valeurs quantitatives
intervals = [0, 30, 80, 150, float('inf')]
labels = ['Groupe 1', 'Groupe 2', 'Groupe 3', 'Groupe 4']

# Utilisez la méthode cut pour créer une nouvelle colonne de valeurs qualitatives
df['writing score'] = pd.cut(df['writing score'], bins=intervals, labels=labels)

# Affichez la DataFrame résultante
print(df)

df['writing score']

import pandas as pd


dt = pd.DataFrame(data)

# Utiliser la fonction get_dummies de Pandas pour créer un tableau disjonctif complet
df_dummies = pd.get_dummies(dt, ['gender', 'race/ethnicity', 'parental_level_of_education','lunch','test_preparation_course','math score','reading score','writing score'])

# Affichez le tableau disjonctif complet
print(df_dummies)

"""***Tableau disjonctif complet***"""

df_dummies

import numpy as np
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, ward, fcluster
import matplotlib.pyplot as plt



df_dummies= np.array(df_dummies)

# Calcul de la dissimilarité de Jaccard
def jaccard_distance(u, v):
    intersection = np.sum(np.logical_and(u, v))
    union = np.sum(np.logical_or(u, v))
    return 1.0 - (intersection / union)

d = pdist(df_dummies, metric=jaccard_distance)
d = squareform(d)

# Regroupement hiérarchique avec la méthode "ward"
ag = ward(d)

# Couper l'arbre en 2 clusters
clusters = fcluster(ag, t=2, criterion='maxclust')

# Afficher les clusters
print(clusters)

# Afficher le dendrogramme
dendrogram(ag)
plt.show()

d

import numpy as np
import numpy.linalg as alg
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.metrics import silhouette_samples, silhouette_score

"""**Pour** **2** **classes**"""

# nombre max de classes =2
clust=fcluster(ag, t=2, criterion='maxclust')
print(clust)

# S indice de silhouette
S= silhouette_samples(df_dummies, clust, metric='euclidean')
print(S)

largeurS= silhouette_score(df_dummies, clust, metric='euclidean')
print(largeurS)

"""**Pour 3 classes**"""

# nombre max de classes =3
clust=fcluster(ag, t=3, criterion='maxclust')
print(clust)

# S indice de silhouette
S= silhouette_samples(df_dummies, clust, metric='euclidean')
print(S)

largeurS= silhouette_score(df_dummies, clust, metric='euclidean')
print(largeurS)

"""# **kmeans**"""

import numpy as np
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
from sklearn.metrics import silhouette_samples, silhouette_score

kmeans = KMeans(n_clusters=2, init='k-means++').fit(df_dummies)
Idx=kmeans.labels_
C=kmeans.cluster_centers_
print(Idx)
print(C)

# S indice de silhouette
S= silhouette_samples(df_dummies, Idx, metric='euclidean')
print(S)
largeurS= silhouette_score(df_dummies, Idx, metric='euclidean')
print(largeurS)

from yellowbrick.cluster import SilhouetteVisualizer


visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')

visualizer.fit(df_dummies)        # ajuster les données pour visualizer
visualizer.show()        # finaliser et afficher la figure

#Elbow method
from yellowbrick.cluster import KElbowVisualizer
model = KMeans(n_init=10)

# k est la plage du nombre de clusters.
visualizer = KElbowVisualizer(model, k=(1,30),timings=False)
visualizer.fit(df_dummies)        # Adapter les données à la visualisation
visualizer.show()        # afficher la figure